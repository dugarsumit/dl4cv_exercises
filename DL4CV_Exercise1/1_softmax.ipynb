{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Classification with Softmax Loss\n",
    "\n",
    "In this exercise you will:\n",
    "\n",
    "- implement a fully-vectorized **loss function** for the Softmax classifier\n",
    "- implement the fully-vectorized expression for its **analytic gradient**\n",
    "- **check your implementation** with numerical gradient\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **SGD**\n",
    "- **visualize** the final learned weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sumit/Documents/tum/deepLearningInComputerVision/dl4cv_exercises\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "No module named dl4cv.model_savers",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-dfa28233dab5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m#import sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#sys.path.append('/home/sumit/Documents/tum/deepLearningInComputerVision/dl4cv_exercises/DL4CV_Exercise1')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdl4cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_savers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_softmax_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dl4cv.model_savers"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "#import sys \n",
    "#sys.path.append('/home/sumit/Documents/tum/deepLearningInComputerVision/dl4cv_exercises/DL4CV_Exercise1')\n",
    "from dl4cv.model_savers import save_softmax_classifier\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# for auto-reloading extenrnal modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CIFAR-10 Data Loading and Preprocessing\n",
    "\n",
    "To your convenience, we have taken care of all the input handling. Nevertheless, you should go through the code line by line so that you understand the general preprocessing pipeline.\n",
    "The whole datasat is loaded, then subdivided into a training, validation and test dataset (the last one is different from the final evaluation dataset on our server!).\n",
    "Before proceeding you should *always* take a look at some samples of your dataset, which is already implemented for you. This way you can make sure that the data input/preprocessing has worked as intended and you can get a feeling for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dl4cv.data_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-9a43774e25e6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdl4cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# Load the raw CIFAR-10 data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcifar10_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'datasets/'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_CIFAR10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcifar10_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dl4cv.data_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from dl4cv.data_utils import load_CIFAR10\n",
    "# Load the raw CIFAR-10 data\n",
    "cifar10_dir = 'datasets/'\n",
    "X, y = load_CIFAR10(cifar10_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y_hat, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y == y_hat)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y_hat + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Split the data into train, val, and test sets. In addition we will\n",
    "# create a small development set as a subset of the data set;\n",
    "# we can use this for development so our code runs faster.\n",
    "num_training = 48000\n",
    "num_validation = 1000\n",
    "num_test = 1000\n",
    "num_dev = 500\n",
    "\n",
    "assert (num_training + num_validation + num_test) == 50000, 'You have not provided a valid data split.'\n",
    "\n",
    "# Our training set will be the first num_train points from the original\n",
    "# training set.\n",
    "mask = range(num_training)\n",
    "X_train = X[mask]\n",
    "y_train = y[mask]\n",
    "\n",
    "\n",
    "# Our validation set will be num_validation points from the original\n",
    "# training set.\n",
    "mask = range(num_training, num_training + num_validation)\n",
    "X_val = X[mask]\n",
    "y_val = y[mask]\n",
    "\n",
    "# We use a small subset of the training set as our test set.\n",
    "mask = range(num_training + num_validation, num_training + num_validation + num_test)\n",
    "X_test = X[mask]\n",
    "y_test = y[mask]\n",
    "\n",
    "# We will also make a development set, which is a small subset of\n",
    "# the training set. This way the development cycle is faster.\n",
    "mask = np.random.choice(num_training, num_dev, replace=False)\n",
    "X_dev = X_train[mask]\n",
    "y_dev = y_train[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "X_dev = np.reshape(X_dev, (X_dev.shape[0], -1))\n",
    "\n",
    "# As a sanity check, print out the shapes of the data\n",
    "print 'Training data shape: ', X_train.shape\n",
    "print 'Validation data shape: ', X_val.shape\n",
    "print 'Test data shape: ', X_test.shape\n",
    "print 'dev data shape: ', X_dev.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Preprocessing: subtract the mean image\n",
    "# first: compute the image mean based on the training data\n",
    "mean_image = np.mean(X_train, axis=0)\n",
    "print mean_image[:10] # print a few of the elements\n",
    "plt.figure(figsize=(4,4))\n",
    "plt.imshow(mean_image.reshape((32,32,3)).astype('uint8')) # visualize the mean image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# second: subtract the mean image from train and test data\n",
    "X_train -= mean_image\n",
    "X_val -= mean_image\n",
    "X_test -= mean_image\n",
    "X_dev -= mean_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# third: append the bias dimension of ones (i.e. bias trick) so that our classifier\n",
    "# only has to worry about optimizing a single weight matrix W.\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "X_dev = np.hstack([X_dev, np.ones((X_dev.shape[0], 1))])\n",
    "\n",
    "print X_train.shape, X_val.shape, X_test.shape, X_dev.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Softmax Classifier\n",
    "\n",
    "Your code for this section will all be written inside `dl4cv/classifiers/softmax.py`. \n",
    "You will implement the gradient of the loss function with respect to the classifier's weights. We suggest that you first derive these expressions on paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Implementation\n",
    "First implement the naive softmax loss function with nested loops.\n",
    "Open the file `dl4cv/classifiers/softmax.py` and implement the softmax_loss_naive function.\n",
    "Running this method might take a while..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from dl4cv.classifiers.softmax import softmax_loss_naive\n",
    "import time\n",
    "\n",
    "# Generate a random softmax weight matrix and use it to compute the loss.\n",
    "W = np.random.randn(3073, 10) * 0.0001\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# As a rough sanity check, our loss should be something close to -log(0.1).\n",
    "print 'loss: %f' % loss\n",
    "print 'sanity check: %f' % (-np.log(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 1:\n",
    "Why do we expect our loss to be close to -log(0.1)? Explain briefly.\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Complete the implementation of softmax_loss_naive and implement a (naive)\n",
    "# version of the gradient that uses nested loops.\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 0.0)\n",
    "\n",
    "# We use numeric gradient checking as a debugging tool.\n",
    "# The numeric gradient should be close to the analytic gradient.\n",
    "from dl4cv.gradient_check import grad_check_sparse\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 0.0)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, num_checks=3)\n",
    "\n",
    "# Do another gradient check with regularization\n",
    "loss, grad = softmax_loss_naive(W, X_dev, y_dev, 1e2)\n",
    "f = lambda w: softmax_loss_naive(w, X_dev, y_dev, 1e2)[0]\n",
    "grad_numerical = grad_check_sparse(f, W, grad, num_checks=3)\n",
    "\n",
    "# Again, running this might take a while!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorized Implementation\n",
    "Now that we have a naive implementation of the softmax loss function and its gradient, implement a vectorized version in softmax_loss_vectorized.\n",
    "The two versions should compute the same results, but the vectorized version should be much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "loss_naive, grad_naive = softmax_loss_naive(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'naive loss: %e computed in %fs' % (loss_naive, toc - tic)\n",
    "\n",
    "from dl4cv.classifiers.softmax import softmax_loss_vectorized\n",
    "tic = time.time()\n",
    "loss_vectorized, grad_vectorized = softmax_loss_vectorized(W, X_dev, y_dev, 0.00001)\n",
    "toc = time.time()\n",
    "print 'vectorized loss: %e computed in %fs' % (loss_vectorized, toc - tic)\n",
    "\n",
    "# We use the Frobenius norm to compare the two versions\n",
    "# of the gradient.\n",
    "grad_difference = np.linalg.norm(grad_naive - grad_vectorized, ord='fro')\n",
    "print 'Loss difference: %f' % np.abs(loss_naive - loss_vectorized)\n",
    "print 'Gradient difference: %f' % grad_difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 2:\n",
    "When you compute the softmax distribution, you are dividing by a sum of exponentials, i.e. potentially very large numbers, which can be numerically unstable. Do you see a way to avoid this problem?\n",
    "\n",
    "(Hint: exploit properties of the exponential function to arrive at an expression that is mathematically the same, but numerically more stable)\n",
    "\n",
    "**Your answer:** *Fill this in*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent\n",
    "\n",
    "We now have vectorized and efficient expressions for the loss, the gradient and our gradient matches the numerical gradient. We are therefore ready to do SGD to minimize the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# In the file linear_classifier.py, implement SGD in the function\n",
    "# LinearClassifier.train() and then run it with the code below.\n",
    "from dl4cv.classifiers import Softmax\n",
    "softmax = Softmax()\n",
    "tic = time.time()\n",
    "loss_hist = softmax.train(X_train, y_train, learning_rate=1e-7, reg=5e4,\n",
    "                      num_iters=1500, verbose=True)\n",
    "toc = time.time()\n",
    "print 'That took %fs' % (toc - tic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# A useful debugging strategy is to plot the loss as a function of\n",
    "# iteration number:\n",
    "plt.figure(figsize=(6,5))\n",
    "plt.plot(loss_hist)\n",
    "plt.xlabel('Iteration number')\n",
    "plt.ylabel('Loss value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Write the LinearClassifier.predict function and evaluate the performance on both the\n",
    "# training and validation set\n",
    "y_train_pred = softmax.predict(X_train)\n",
    "print 'training accuracy: %f' % (np.mean(y_train == y_train_pred), )\n",
    "y_val_pred = softmax.predict(X_val)\n",
    "print 'validation accuracy: %f' % (np.mean(y_val == y_val_pred), )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training your Softmax Classifier\n",
    "Use the validation set to tune hyperparameters (regularization strength, learning rate and possibly batch size). You should experiment with different ranges for the learning rates and regularization strengths; if you are careful you should be able to get a classification accuracy of over 0.35 on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 / 3000: loss 22.058980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 100 / 3000: loss 11.087819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 200 / 3000: loss 6.861756\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 300 / 3000: loss 4.768020\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 400 / 3000: loss 3.651193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 500 / 3000: loss 2.908850\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 600 / 3000: loss 2.410140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 700 / 3000: loss 2.136017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 800 / 3000: loss 2.062980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 900 / 3000: loss 2.067118\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1000 / 3000: loss 1.910647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1100 / 3000: loss 1.912642\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1200 / 3000: loss 1.761087\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1300 / 3000: loss 1.936542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1400 / 3000: loss 1.849176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1500 / 3000: loss 1.819100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1600 / 3000: loss 1.755033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1700 / 3000: loss 1.678619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1800 / 3000: loss 1.728173\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1900 / 3000: loss 1.770656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2000 / 3000: loss 1.817448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2100 / 3000: loss 1.692310\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2200 / 3000: loss 1.792194\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2300 / 3000: loss 1.878828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2400 / 3000: loss 1.919706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2500 / 3000: loss 1.744315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2600 / 3000: loss 1.821214\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2700 / 3000: loss 1.694280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2800 / 3000: loss 1.748335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2900 / 3000: loss 1.766433\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lr 2.700000e-06 reg 1.000000e+03 train accuracy: 0.406604 val accuracy: 0.400000\nbest validation accuracy achieved during validation: 0.400000\n"
     ]
    }
   ],
   "source": [
    "from dl4cv.classifiers import Softmax\n",
    "\n",
    "# results is dictionary mapping tuples of the form\n",
    "# (learning_rate, regularization_strength) to tuples of the form\n",
    "# (training_accuracy, validation_accuracy). The accuracy is simply the fraction\n",
    "# of data points that are correctly classified.\n",
    "results = {}\n",
    "best_val = -1\n",
    "best_softmax = None\n",
    "learning_rates = [2.7e-6]\n",
    "regularization_strengths = [1e3]\n",
    "#learning_rates = np.logspace(-10, 10, 10)\n",
    "#regularization_strengths = np.logspace(-3, 6, 10)\n",
    "\n",
    "################################################################################\n",
    "# TODO:                                                                        #\n",
    "# Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "# set. For each combination of hyperparameters, train a classifier on the      #\n",
    "# training set, compute its accuracy on the training and validation sets, and  #\n",
    "# store these numbers in the results dictionary. In addition, store the best   #\n",
    "# validation accuracy in best_val and the Softmax object that achieves this    #\n",
    "# accuracy in best_softmax.                                                    #\n",
    "#                                                                              #\n",
    "# Hint: You should use a small value for num_iters as you develop your         #\n",
    "# validation code so that the classifiers don't take much time to train;       # \n",
    "# once you are confident that your validation code works, you should rerun     #\n",
    "# the validation code with a larger value for num_iters.                       #\n",
    "################################################################################\n",
    "pass\n",
    "for lr in learning_rates:\n",
    "    for reg in regularization_strengths:\n",
    "        softmax_obj = Softmax()\n",
    "        softmax_obj.train(X_train, y_train, learning_rate=lr, reg=reg, \n",
    "                      num_iters=3000, batch_size=200, verbose=True)\n",
    "        \n",
    "        pred_y_train = softmax_obj.predict(X_train)\n",
    "        pred_y_val = softmax_obj.predict(X_val)\n",
    "        accuracy_train = np.mean(y_train == pred_y_train)\n",
    "        accuracy_val = np.mean(y_val == pred_y_val)\n",
    "        results[(lr, reg)] = (accuracy_train, accuracy_val)\n",
    "        \n",
    "        if accuracy_val > best_val:\n",
    "            best_val = accuracy_val\n",
    "            best_softmax = softmax_obj\n",
    "################################################################################\n",
    "#                              END OF YOUR CODE                                #\n",
    "################################################################################\n",
    "    \n",
    "# Print out results.\n",
    "for lr, reg in sorted(results):\n",
    "    train_accuracy, val_accuracy = results[(lr, reg)]\n",
    "    print 'lr %e reg %e train accuracy: %f val accuracy: %f' % (\n",
    "                lr, reg, train_accuracy, val_accuracy)\n",
    "    \n",
    "print 'best validation accuracy achieved during validation: %f' % best_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo4AAAI4CAYAAADknWiIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\njIiIiIhKkjhGRERERCVJHCMiIiKikv8PB1nUODrKv6EAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2a56768350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the validation results\n",
    "import math\n",
    "x_scatter = [math.log10(x[0]) for x in results]\n",
    "y_scatter = [math.log10(x[1]) for x in results]\n",
    "\n",
    "# plot training accuracy\n",
    "marker_size = 100\n",
    "colors = [results[x][0] for x in results]\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 training accuracy')\n",
    "\n",
    "# plot validation accuracy\n",
    "colors = [results[x][1] for x in results] # default size of markers is 20\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.scatter(x_scatter, y_scatter, marker_size, c=colors)\n",
    "plt.colorbar()\n",
    "plt.xlabel('log learning rate')\n",
    "plt.ylabel('log regularization strength')\n",
    "plt.title('CIFAR-10 validation accuracy')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "softmax on raw pixels final test set accuracy: 0.421000\n"
     ]
    }
   ],
   "source": [
    "# evaluate on test set\n",
    "# Evaluate the best softmax on test set\n",
    "y_test_pred = best_softmax.predict(X_test)\n",
    "test_accuracy = np.mean(y_test == y_test_pred)\n",
    "print 'softmax on raw pixels final test set accuracy: %f' % (test_accuracy, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize the learned weights for each class\n",
    "w = best_softmax.W[:-1,:] # strip out the bias\n",
    "w = w.reshape(32, 32, 3, 10)\n",
    "\n",
    "w_min, w_max = np.min(w), np.max(w)\n",
    "\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "for i in xrange(10):\n",
    "  plt.subplot(2, 5, i + 1)\n",
    "  \n",
    "  # Rescale the weights to be between 0 and 255\n",
    "  wimg = 255.0 * (w[:, :, :, i].squeeze() - w_min) / (w_max - w_min)\n",
    "  plt.imshow(wimg.astype('uint8'))\n",
    "  plt.axis('off')\n",
    "  plt.title(classes[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inline Question 3:\n",
    "Describe what your visualized Softmax weights look like.\n",
    "\n",
    "**Your answer:** *fill this in*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save the model\n",
    "\n",
    "When you are satisfied with your training, save the model for submission.\n",
    "\n",
    "**Note**: You might get an error like this:\n",
    "\n",
    "PicklingError: Can't pickle `<class 'dl4cv.classifiers.linear_classifier.Softmax'>`: it's not the same object as dl4cv.classifiers.linear_classifier.Softmax\n",
    "\n",
    "The reason is that we are using autoreload and working on this class during the notebook session. If you get this error simply restart the kernel and rerun the whole script (Kernel -> Restart & Run All) or only the important cells for generating your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dl4cv.model_savers",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4e8cd7d3530d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdl4cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_savers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msave_softmax_classifier\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0msave_softmax_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_softmax\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dl4cv.model_savers"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from dl4cv.model_savers import save_softmax_classifier\n",
    "save_softmax_classifier(best_softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring function\n",
    "We will score the model you have just saved based on the classification accuracy on our test dataset. The scoring function should represent the difficulty of obtaining a good test accuracy and should therefore give 0 points for worse results than random guessing, should be linear in a first regime and exponential beyond that. The onset of exponential growth depends on the problem. In that region you get twice as many points for an additional 10% accuracy.\n",
    "\n",
    "For this problem we specifically use the following scoring function:\n",
    "\n",
    "$$f(x) = \\left\\{\n",
    "\t\\begin{array}{ll}\n",
    "\t\t0  & \\mbox{if } x \\leq 0.1 \\\\\n",
    "\t\t100x & \\mbox{if } 0.1 < x \\leq 0.4 \\\\\n",
    "        \\left(\\frac{40}{\\exp(0.4 \\ln(2)/0.1)}\\right) \\exp(x \\ln(2)/0.1) & \\mbox{if } 0.4 < x \\leq 1\n",
    "\t\\end{array}\n",
    "\\right.$$\n",
    "\n",
    "The function can be plotted in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "No module named dl4cv.data_utils",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mImportError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-d811f6037131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdl4cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscoring_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlin_exp_boundary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoubling_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Scoring Function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: No module named dl4cv.data_utils"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "from dl4cv.data_utils import scoring_function\n",
    "\n",
    "x = np.linspace(0, 1, num=1000)\n",
    "plt.plot(x, scoring_function(x, lin_exp_boundary=0.4, doubling_rate=0.1))\n",
    "plt.title('Scoring Function')\n",
    "plt.xlabel('Test Accuracy')\n",
    "plt.ylabel('Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}